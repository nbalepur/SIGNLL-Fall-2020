{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/8 Notebook - Customer Support Chatbot (Part A)\n",
    "\n",
    "Hello and welcome to this week's notebook! Today, we'll be looking at how to create our own, customizable chat bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the methods you need to complete for the notebook:\n",
    "1. asd\n",
    "2. asd\n",
    "3. asd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing our libraries as always. Make sure you run the cell with `pip install nltk`, which will let you download the `nltk` library we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nishu\\anaconda3\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\nishu\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import our nltk libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# install specific downloads\n",
    "nltk.download('punkt', quiet = True)\n",
    "nltk.download('wordnet', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other useful libraries (numpy == ðŸ)\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Modify your intents\n",
    "\n",
    "The great part about this chat bot is that it is fully customizable! Edit `intents.json` to your liking to create your own bot. Make sure that for each `intent`, you fill out the fields `tag`, `patterns`, and `responses`\n",
    "\n",
    "You can look at my file, `taco-bell-intents.json`, for reference\n",
    "\n",
    "Once you're done, you can continue to run the cells below!\n",
    "\n",
    "**Note: if you're having JSON formatting issues in the next cell, use [this link](https://jsonlint.com) to validate your JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hi there', 'How are you', 'Is anyone there?', 'Hey', 'Hola', 'Hello', 'Good day'], 'responses': ['Hello, thanks for asking', 'Good to see you again', 'Hi there, how can I help?']}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye', 'Nice chatting to you, bye', 'Till next time'], 'responses': ['See you!', 'Have a nice day', 'Bye! Come back again soon.']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\", 'Awesome, thanks', 'Thanks for helping me'], 'responses': ['Happy to help!', 'Any time!', 'My pleasure']}, {'tag': 'noanswer', 'patterns': [], 'responses': [\"Sorry, can't understand you\", 'Please give me more info', 'Not sure I understand']}, {'tag': 'options', 'patterns': ['How you could help me?', 'What you can do?', 'What help you provide?', 'How you can be helpful?', 'What support is offered'], 'responses': [\"I can direct you to your nearest Taco Bell, send you contact information, give you a recommendation, tell you today's deals, and give you a fun fact!\", 'Offering support for directions, contact information, recommendations, deals, and fun facts!']}, {'tag': 'directions', 'patterns': ['Can you give me directions?', 'How do I get there?', 'Where are you located?', 'How do I find your location?', 'Tell me where you are'], 'responses': ['We are located at 512 E Green Street', 'Come visit us at 512 E Green Street', 'You can find us on Green Street, near Legends and Signature Grill']}, {'tag': 'contact', 'patterns': ['How do I contact you?', \"What's your phone number\", 'How do I call you?', 'Tell me your contact information'], 'responses': ['Our phone number is 217-344-9649', 'Call us at 217-344-9649!', 'You can reach us at 217-344-9649']}, {'tag': 'recommendation', 'patterns': ['Do you have any recommendations?', 'What should I eat today?', 'What is your best menu item?', 'Give me a recommendation', 'Give me something to eat'], 'responses': ['Try our Shredded Chicken Burrito!', 'Try our XXL Grilled Stuffed Burrito!', 'Try our Mexican Pizza!', 'Try our Quesarito!', 'Try our Loaded Potato Griller!', 'Try our Enchirito!', \"Try our Pintos 'n Cheese!\", 'Try our Cheesy Fiesta Potatoes!', 'Try our Cinna Twists!', 'Try our 7-Layer Burrito!', 'Try our Grande Scrambler!', 'Try our Nachos Supreme!', 'Try our Double Chalupa!', 'Try our Doritos Locos Tacos!', 'Try our Chili Cheese Burrito!', 'Try our Cheesy Gordita Crunch!', 'Try our Crunchwrap Supreme!']}, {'tag': 'deals', 'patterns': ['What are the deals for today?', 'Do you have any daily specials?', \"What's the deal of the day?\", 'Do you have any discounts?', 'Is there anything new on the menu?'], 'responses': ['Our $5 Grande Stacker Box might be worth your time!', 'If you have a large group, the Nachos Party Pack is what you need!', 'Try our new Dragonfruit Freeze drink!']}, {'tag': 'fact', 'patterns': ['Give me a fun fact', 'Tell me something new', 'Can you tell me a fun fact?'], 'responses': ['Taco Bell is named after our founder, Glen Bell', 'Our tacos were originally 19 cents!', 'The first Taco Bell location had fire pits and mariachi bands', 'The taco bell Chihuahua made us have to settle a 42 million dollar lawsuit!', 'We are so so so so sorry for removing the Quesarito :(']}]}\n"
     ]
    }
   ],
   "source": [
    "data_file = open(\"intents.json\").read()\n",
    "intents = json.loads(data_file)\n",
    "# when you print, you should see your JSON\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing the JSON\n",
    "\n",
    "We'll practice a common first step in any NLP project, data cleaning\n",
    "\n",
    "First, complete the function `process_words()` which will clean up our words according to the following steps:\n",
    "1. Get the tokens using `nltk.word_tokenize()`\n",
    "2. Set `cleaned_word` equal to the `lemmatized` and `lowercased` word\n",
    "\n",
    "**Note: Make sure you run the cell immediately below this first; it stores values needed in `process_words()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare needed variables for process_words()\n",
    "ignore_punctuation = [\"?\", \"!\", \".\", \",\"]\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(pattern):\n",
    "    # return variable\n",
    "    words = []\n",
    "    # get the tokens using nltk\n",
    "    tokens = nltk.word_tokenize(pattern)\n",
    "    for word in tokens:\n",
    "        # check if the word should be ignored\n",
    "        if word not in ignore_punctuation and word.isalnum():\n",
    "            # clean the word and add it to the list\n",
    "            cleaned_word = lemmatizer.lemmatize(word.lower())\n",
    "            words.append(cleaned_word)\n",
    "    # return the list\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CELL FOR TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `process_words()` to clean our words, we can parse the data from our JSON\n",
    "\n",
    "Complete the method `parse_intens()` which parses our JSON according to the following steps:\n",
    "1. Set the value of `tag` from our `intent`\n",
    "2. Set `tokenized_words` using the helper method in `process_words()`\n",
    "3. Append a tuple of `tokenized_words` and `tag` to `tag_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_intents(intents):\n",
    "    # declare our needed variables\n",
    "    tags = []\n",
    "    all_words = []\n",
    "    tag_tokens = []\n",
    "    # iterate through each intent\n",
    "    for intent in intents[\"intents\"]:\n",
    "        # if the intent has no patterns, we can skip\n",
    "        if (len(intent[\"patterns\"]) == 0):\n",
    "            continue\n",
    "        # add the tag to the list of tag\n",
    "        tag = intent[\"tag\"]\n",
    "        tags.append(tag)\n",
    "        # iterate through each pattern\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            # create our tokenized words\n",
    "            tokenized_words = process_words(pattern)\n",
    "            # add all the tokenized words to our words\n",
    "            all_words.extend(tokenized_words)\n",
    "            # adds a tuple -> (list of tokens, tag) -> to the list\n",
    "            tag_tokens.append((tokenized_words, tag))\n",
    "    # return our values in a tuple\n",
    "    return (np.array(tags), np.array(all_words), np.array(tag_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this cool trick below to remove all duplicates from our arrays (and sort them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags, all_words, tag_tokens = parse_intents(intents)\n",
    "\n",
    "tags = np.array(sorted(list(set(tags))))\n",
    "all_words = np.array(sorted(list(set(all_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below and take a quick look to make sure that everything makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: ['contact' 'deals' 'directions' 'fact' 'goodbye' 'greeting' 'options'\n",
      " 'recommendation' 'thanks']\n",
      "------\n",
      "All Words: ['a' 'any' 'anyone' 'anything' 'are' 'awesome' 'be' 'best' 'bye' 'call'\n",
      " 'can' 'chatting' 'contact' 'could' 'daily' 'day' 'deal' 'direction'\n",
      " 'discount' 'do' 'eat' 'fact' 'find' 'for' 'fun' 'get' 'give' 'good'\n",
      " 'goodbye' 'have' 'hello' 'help' 'helpful' 'helping' 'hey' 'hi' 'hola'\n",
      " 'how' 'i' 'information' 'is' 'item' 'later' 'located' 'location' 'me'\n",
      " 'menu' 'new' 'next' 'nice' 'number' 'of' 'offered' 'on' 'phone' 'provide'\n",
      " 'recommendation' 'see' 'should' 'something' 'special' 'support' 'tell'\n",
      " 'thank' 'thanks' 'that' 'the' 'there' 'till' 'time' 'to' 'today' 'what'\n",
      " 'where' 'you' 'your']\n",
      "------\n",
      "Tag-Token Mappings [[list(['hi', 'there']) 'greeting']\n",
      " [list(['how', 'are', 'you']) 'greeting']\n",
      " [list(['is', 'anyone', 'there']) 'greeting']\n",
      " [list(['hey']) 'greeting']\n",
      " [list(['hola']) 'greeting']\n",
      " [list(['hello']) 'greeting']\n",
      " [list(['good', 'day']) 'greeting']\n",
      " [list(['bye']) 'goodbye']\n",
      " [list(['see', 'you', 'later']) 'goodbye']\n",
      " [list(['goodbye']) 'goodbye']\n",
      " [list(['nice', 'chatting', 'to', 'you', 'bye']) 'goodbye']\n",
      " [list(['till', 'next', 'time']) 'goodbye']\n",
      " [list(['thanks']) 'thanks']\n",
      " [list(['thank', 'you']) 'thanks']\n",
      " [list(['that', 'helpful']) 'thanks']\n",
      " [list(['awesome', 'thanks']) 'thanks']\n",
      " [list(['thanks', 'for', 'helping', 'me']) 'thanks']\n",
      " [list(['how', 'you', 'could', 'help', 'me']) 'options']\n",
      " [list(['what', 'you', 'can', 'do']) 'options']\n",
      " [list(['what', 'help', 'you', 'provide']) 'options']\n",
      " [list(['how', 'you', 'can', 'be', 'helpful']) 'options']\n",
      " [list(['what', 'support', 'is', 'offered']) 'options']\n",
      " [list(['can', 'you', 'give', 'me', 'direction']) 'directions']\n",
      " [list(['how', 'do', 'i', 'get', 'there']) 'directions']\n",
      " [list(['where', 'are', 'you', 'located']) 'directions']\n",
      " [list(['how', 'do', 'i', 'find', 'your', 'location']) 'directions']\n",
      " [list(['tell', 'me', 'where', 'you', 'are']) 'directions']\n",
      " [list(['how', 'do', 'i', 'contact', 'you']) 'contact']\n",
      " [list(['what', 'your', 'phone', 'number']) 'contact']\n",
      " [list(['how', 'do', 'i', 'call', 'you']) 'contact']\n",
      " [list(['tell', 'me', 'your', 'contact', 'information']) 'contact']\n",
      " [list(['do', 'you', 'have', 'any', 'recommendation']) 'recommendation']\n",
      " [list(['what', 'should', 'i', 'eat', 'today']) 'recommendation']\n",
      " [list(['what', 'is', 'your', 'best', 'menu', 'item']) 'recommendation']\n",
      " [list(['give', 'me', 'a', 'recommendation']) 'recommendation']\n",
      " [list(['give', 'me', 'something', 'to', 'eat']) 'recommendation']\n",
      " [list(['what', 'are', 'the', 'deal', 'for', 'today']) 'deals']\n",
      " [list(['do', 'you', 'have', 'any', 'daily', 'special']) 'deals']\n",
      " [list(['what', 'the', 'deal', 'of', 'the', 'day']) 'deals']\n",
      " [list(['do', 'you', 'have', 'any', 'discount']) 'deals']\n",
      " [list(['is', 'there', 'anything', 'new', 'on', 'the', 'menu']) 'deals']\n",
      " [list(['give', 'me', 'a', 'fun', 'fact']) 'fact']\n",
      " [list(['tell', 'me', 'something', 'new']) 'fact']\n",
      " [list(['can', 'you', 'tell', 'me', 'a', 'fun', 'fact']) 'fact']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tags: {0}\".format(tags))\n",
    "print(\"------\")\n",
    "print(\"All Words: {0}\".format(all_words))\n",
    "print(\"------\")\n",
    "print(\"Tag-Token Mappings {0}\".format(tag_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a Training Set\n",
    "\n",
    "We know from previous lessons that the computer can't train a model without numeric values. To solve this, we'll use the `bag of words` technique we discussed in the Google Sheets\n",
    "\n",
    "Complete the method `build_training_set()` below, which performs the following steps:\n",
    "1. asd\n",
    "2. asd\n",
    "3. asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_set(tags, all_words, tag_tokens):\n",
    "    # define our variables to return\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "        \n",
    "    # iterate through each tag-token mapping\n",
    "    for tag_token in tag_tokens:\n",
    "        \n",
    "        # grab our needed values\n",
    "        tokens = tag_token[0]\n",
    "        tag = tag_token[1]\n",
    "        \n",
    "        # reset our current bag\n",
    "        current_bag = []\n",
    "    \n",
    "        for word in all_words:\n",
    "            # add 0/1 if the word is in our token\n",
    "            in_token = (word in tokens)\n",
    "            current_bag.append(1 * in_token)\n",
    "            \n",
    "        # update our training inputs\n",
    "        train_x.append(current_bag)\n",
    "        \n",
    "        # set our outputs equal to 1 in the locatio\n",
    "        train_y.append(1 * (tags == tag))\n",
    "    \n",
    "    # return our values\n",
    "    return (np.array(train_x), np.array(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_training_set(tags, all_words, tag_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
