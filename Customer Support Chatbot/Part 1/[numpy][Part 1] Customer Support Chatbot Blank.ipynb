{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/8 Notebook - Customer Support Chatbot (Part A)\n",
    "\n",
    "Hello and welcome to this week's notebook! Today, we'll be looking at how to create our own, customizable chat bot. Specifically, we'll be creating a custom data set, learning how to professionally clean data, and training a chat bot using a bag-of-words model\n",
    "\n",
    "**Note: This notebook does NOT require any additional installation of `Keras` and `Tensorflow`. If you want to get some experience with these libaries, check out the other notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the methods you need to complete for the notebook:\n",
    "1. Edit `intents.json`\n",
    "2. `process_words()`\n",
    "3. `parse_intents()`\n",
    "4. `build_bag()`\n",
    "5. `build_training_set()`\n",
    "6. `test_accuracy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing our libraries as always. Make sure you run the cell with `pip install nltk`, which will let you download the `nltk` library we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our nltk libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# install specific downloads\n",
    "nltk.download('punkt', quiet = True)\n",
    "nltk.download('wordnet', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other useful libraries (numpy == üêê)\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Modify your intents\n",
    "\n",
    "The great part about this chat bot is that it is fully customizable! Edit `intents.json` to your liking to create your own bot. Make sure that for each `intent`, you fill out the fields `tag`, `patterns`, and `responses`\n",
    "\n",
    "You can look at my file, `taco-bell-intents.json`, for reference\n",
    "\n",
    "Once you're done, you can continue to run the cells below!\n",
    "\n",
    "**Note: if you're having JSON formatting issues in the next cell, use [this link](https://jsonlint.com) to validate your JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"intents.json\").read()\n",
    "intents = json.loads(data_file)\n",
    "# when you print, you should see your JSON\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing the JSON\n",
    "\n",
    "We'll practice a common first step in any NLP project, data cleaning\n",
    "\n",
    "First, complete the function `process_words()` which will clean up our words according to the following steps:\n",
    "1. Get the tokens using `nltk.word_tokenize()`\n",
    "2. Set `cleaned_word` equal to the `lemmatized` and `lowercased` word\n",
    "\n",
    "**Note: Make sure you run the cell immediately below this first; it stores values needed in `process_words()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Set <code>tokens = nltk.word_tokenize(pattern)</code></li>\n",
    "    <li><code>lemmatizer.lemmatize(...)</code> will lemmatize a word</li>\n",
    "    <li>The paremeter of <code>lemmatizer.lemmatize(...)</code> should be <code>word.lower()</code></li>  \n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare needed variables for process_words()\n",
    "ignore_punctuation = [\"?\", \"!\", \".\", \",\"]\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(pattern):\n",
    "    # return variable\n",
    "    words = []\n",
    "    # [your code here] - get the tokens using nltk\n",
    "    tokens = ...\n",
    "    for word in tokens:\n",
    "        # check if the word should be ignored\n",
    "        if word not in ignore_punctuation and word.isalnum():\n",
    "            # [your code here] - clean the word and add it to the list\n",
    "            cleaned_word = ...\n",
    "            words.append(cleaned_word)\n",
    "    # return the list\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "if (process_words(\"How was your day today?\") == ['how', 'wa', 'your', 'day', 'today']):\n",
    "    print(\"Nice work, sport!\")\n",
    "else:\n",
    "    print(\"Try again, buddy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `process_words()` to clean our words, we can parse the data from our JSON\n",
    "\n",
    "Complete the method `parse_intents()` which does the following:\n",
    "1. Set the value of `tag` from our `intent`\n",
    "2. Set `tokenized_words` using the helper method in `process_words()`\n",
    "3. Append a tuple of `tokenized_words` and `tag` to `tag_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Values of a JSON can be extracted using arrays</li>\n",
    "    <li>Let <code>tag = intent[\"tag\"]</code></li>\n",
    "    <li>Let <code>tokenized_words = process_words(pattern)</code></li>\n",
    "    <li>For the third step, the tuple can be appended with <code>tag_tokens.append((tokenized_words, tag))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_intents(intents):\n",
    "    # declare our needed variables\n",
    "    tags = []\n",
    "    all_words = []\n",
    "    tag_tokens = []\n",
    "    response_dict = dict()\n",
    "    \n",
    "    # iterate through each intent\n",
    "    for intent in intents[\"intents\"]:\n",
    "        # if the intent has no patterns, we can skip\n",
    "        if (len(intent[\"patterns\"]) == 0):\n",
    "            continue\n",
    "        \n",
    "        # [your code here] - add the tag to the list of tag\n",
    "        tag = ...\n",
    "        tags.append(tag)\n",
    "        \n",
    "        # update the dictionary\n",
    "        response_dict[tag] = intent[\"responses\"]\n",
    "        \n",
    "        # iterate through each pattern\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            # [your code here] - create our tokenized words\n",
    "            tokenized_words = ...\n",
    "            # add all the tokenized words to our words\n",
    "            all_words.extend(tokenized_words)\n",
    "            # [your code here] - adds a tuple -> (list of tokens, tag) -> to the list\n",
    "            tag_tokens.append(...)\n",
    "    # return our values in a tuple\n",
    "    return (np.array(tags), np.array(all_words), np.array(tag_tokens), response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this cool trick below to remove all duplicates from our arrays (and sort them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call our function\n",
    "tags, all_words, tag_tokens, tag_responses = parse_intents(intents)\n",
    "# sort and remove duplicates\n",
    "tags = np.array(sorted(list(set(tags))))\n",
    "all_words = np.array(sorted(list(set(all_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below and take a quick look to make sure that everything makes sense. It's hard for me to test your code without knowing what's in your JSON, but in general:\n",
    "\n",
    "- `tags` should contain a list of all your tags in the JSON, excluding `noanswer`\n",
    "- `all_words` should be a list of all the words in your JSON's patterns. There should be no duplicates or patterns that aren't words\n",
    "- Each entry of `tag_token_mappings` should have two values in a list. The first should be a list of patterns, and the second should be the tag of that pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tags: {0}\".format(tags))\n",
    "print(\"------\")\n",
    "print(\"All Words: {0}\".format(all_words))\n",
    "print(\"------\")\n",
    "print(\"Tag-Token Mappings: {0}\".format(tag_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a Training Set\n",
    "\n",
    "We know from previous lessons that the computer can't train a model without numeric values. To solve this, we'll use the `bag of words` technique we discussed in the Google Sheets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the helper method `build_bag()` which iterates through each `word` in `all_words`, and appends 1 to `bag` if the word is in `all_words`, and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The easiest way to do this is by using a simple <code>if else</code> statement</li>\n",
    "    <li>Recall that <code>A in B</code> will return <code>true</code> if the element A is in the iterable object B, and <code>false</code> otherwise</li>\n",
    "    <li>If you're feeling really fancy, you can just write <code>bag.append(1 * (word in tokens))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag(all_words, tokens):\n",
    "    # reset our current bag\n",
    "    bag = []\n",
    "    for word in all_words:\n",
    "        # [your code here] - append the correct value to the bag\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "test_all_words = [\"edgar\", \"allen\", \"poe\", \"said\", \"the\", \"raven\", \"was\", \"nevermore\"]\n",
    "test_tokens = [\"quote\", \"the\", \"raven\", \"nevermore\"]\n",
    "if (build_bag(test_all_words, test_tokens) == [0, 0, 0, 0, 1, 1, 0, 1]):\n",
    "    print(\"You crushed it!\")\n",
    "else:\n",
    "    print(\"Ruh roh raggy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `build_training_set()` below, which performs the following steps:\n",
    "1. Grabs the value of `tokens`, the first (index 0) element of `tag_token`\n",
    "2. Grabs the value of `tag`, the second (index 1) element of `tag_token`\n",
    "3. Sets `current_bag` using the helper method `build_bag()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can get the values of <code>tokens</code> and <code>tag</code> with <code>tag_token[X]</code>, where <code>X</code> is 0 or 1, appropriately</li>\n",
    "    <li>Let <code>current_bag = build_bag(all_words, tokens)</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_set(tags, all_words, tag_tokens):\n",
    "    # define our variables to return\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "        \n",
    "    # iterate through each tag-token mapping\n",
    "    for tag_token in tag_tokens:\n",
    "        \n",
    "        # [your code here] - grab our needed values\n",
    "        tokens = ...\n",
    "        tag = ...\n",
    "        \n",
    "        # [your code here] - reset our current bag\n",
    "        current_bag = ...\n",
    "            \n",
    "        # update our training inputs\n",
    "        train_x.append(current_bag)\n",
    "        \n",
    "        # set our outputs equal to 1 in the location\n",
    "        train_y.append(1 * (tags == tag))\n",
    "    \n",
    "    # return our values\n",
    "    return (np.array(train_x), np.array(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_training_set(tags, all_words, tag_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your `train_x` and `train_y` values in the following cell. It's hard for me to tell if you did everything correctly since you could be using a custom data set. If you have any questions about the program, feel free to message me on discord!\n",
    "\n",
    "- `train_x` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # words in `all_words`\n",
    "- `train_y` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # tags in `tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(\"Training Inputs: {0}\".format(train_x))\n",
    "print(\"-----\")\n",
    "print(\"Training Outputs: {0}\".format(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with training, you may notice that our data is very similarly grouped, specifically the training outputs. As you may have thought, this can cause some unwanted bias in our model. To fix this, we'll `shuffle` our training set by using `np.random.permutation()` and some clever array indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled indexes\n",
    "shuffled_indexes = np.random.permutation(train_x.shape[0])\n",
    "# set new values for train_x and train_y\n",
    "train_x = train_x[shuffled_indexes]\n",
    "train_y = train_y[shuffled_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Our Model from Scratch (no coding until the end)\n",
    "\n",
    "We have our cleaned, numeric inputs and outputs (`train_x` and `train_y`), so now what? \n",
    "\n",
    "It's time to train our model!\n",
    "\n",
    "**Note: In this version of the notebook, we'll be using the `numpy` neural network we developed last week. This neural network isn't as sophisticated as the one that Keras/Tensorflow generates, but you won't have to install any extra packages. If you want some experience working with other packages, check out the other version of the notebook (but know you will need to install Keras/Tensorflow)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will use the following architecture:\n",
    "\n",
    "<img src = \"./bag_of_words.PNG\" style=\"width:75%;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll copy and paste our nifty helper functions from last week, `sigmoid()` and `sigmoid_derivative()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # calculate the output of the sigmoid function and return it\n",
    "    sigmoid_val = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    # calculate the derivative and return the value\n",
    "    sigmoid_deriv = np.multiply(sigmoid(x), 1 - sigmoid(x))\n",
    "    return sigmoid_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our neural networks are broken down into two methods, defined below:\n",
    "1. `forward_prop()`\n",
    "2. `back_prop()`\n",
    "\n",
    "`forward_prop()` makes our predictions for a set of `thetas`, while `back_prop()` makes adjustments to these `thetas`\n",
    "\n",
    "We will combine these functions in `train_model()`, but first let's take a look at `forward_prop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(inputs, thetas, m):\n",
    "    # declare the values we need\n",
    "    outputs = []\n",
    "    curr_inputs = inputs\n",
    "    ones_col = np.ones((m, 1))\n",
    "    for theta in thetas:\n",
    "        # format the inputs by adding the column of ones\n",
    "        formatted_inputs = np.hstack((ones_col, curr_inputs))\n",
    "        # calculate the predicted value, and append it to the list of outputs\n",
    "        pred_val = formatted_inputs @ theta.T\n",
    "        outputs.append(pred_val)\n",
    "        # set curr_inputs to the the sigmoid of our predicted value\n",
    "        curr_inputs = sigmoid(pred_val)\n",
    "    # return our list of outputs\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define `back_prop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(y_predictions, y_actual, inputs, thetas, m, num_classifications):\n",
    "    # sets the constant for ones_col\n",
    "    ones_col = np.ones((m, 1))\n",
    "    \n",
    "    # calculates the \"difference\" for the final layer\n",
    "    diff3 = sigmoid(y_predictions[-1]) - y_actual\n",
    "    \n",
    "    # calculates the \"difference\" for the penultimate layer\n",
    "    diff2_unadjusted = diff3 @ thetas[1][:, 1:]\n",
    "    diff2 = np.multiply(diff2_unadjusted, sigmoid_derivative(y_predictions[0]))\n",
    "    \n",
    "    # formats the partial derivatives\n",
    "    format_partial_one = np.hstack((ones_col, np.asarray(inputs)))\n",
    "    format_partial_two = np.hstack((ones_col, np.asarray(sigmoid(y_predictions[0]))))\n",
    "    \n",
    "    # calculates the unadjusted partial derivatives\n",
    "    delta_one = diff2.T @ format_partial_one\n",
    "    delta_two = diff3.T @ format_partial_two\n",
    "    \n",
    "    # returns our partial derivatives divided by m (to scale)\n",
    "    return [delta_one / m, delta_two / m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll combine them in `train_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(thetas, inputs, actual_outputs, num_iterations, learning_rate, sample_size, num_classifications):\n",
    "    for iteration in range(num_iterations):\n",
    "        # calculate the outputs for the iteration\n",
    "        outputs = forward_prop(inputs, thetas, sample_size)\n",
    "        # calculate the gradients for the iteration\n",
    "        gradients = back_prop(outputs, actual_outputs, inputs, thetas, sample_size, num_classifications)\n",
    "        # adjust both of our thetas, taking a small step towards the minimum\n",
    "        thetas[0] = thetas[0] - learning_rate * gradients[0]\n",
    "        thetas[1] = thetas[1] - learning_rate * gradients[1]\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train our model, we need to define our `constants`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000\n",
    "learning_rate = 0.5\n",
    "sample_size = train_x.shape[0]\n",
    "num_classifications = train_y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as our `thetas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_nodes = 64\n",
    "theta_one = np.random.random((num_hidden_nodes, train_x.shape[1] + 1)) - 0.5\n",
    "theta_two = np.random.random((num_classifications, num_hidden_nodes + 1)) - 0.5\n",
    "thetas = [theta_one, theta_two]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's finally time to `train` our model! ü§û"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = train_model(thetas, train_x, train_y, num_iterations, learning_rate, sample_size, num_classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to make the chatbot GUI, test the model, and other fun stuff (unfortunately next week üò¢), we need to test the `accuracy` of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the formula for `accuracy`:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{200}&space;accuracy&space;=&space;\\frac{n_{correct}}{n_{total}}\" title=\"accuracy = \\frac{n_{correct}}{n_{total}}\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function `test_accuracy()` completes the following steps:\n",
    "\n",
    "1. Calculate `max_inputs` and `max_outputs` by using `np.argmax()` and setting the `axis` parameter equal to 1\n",
    "2. Calculate `num_correct`\n",
    "3. Calculate `accuracy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use <code>np.argmax(A, axis = 1)</code> for <code>max_inputs</code> and <code>max_outputs</code>, where <code>A</code> is the array you want to find the max of</li>\n",
    "    <li><code>num_correct</code> can be found by using <code>np.sum()</code> with the values where <code>max_inputs == max_outputs</code></li>\n",
    "    <li>Use the formula to calculate <code>accuracy</code>!</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(x, y, thetas):\n",
    "    # get our outputs by forward propogating\n",
    "    outputs = sigmoid(forward_prop(x, thetas, x.shape[0])[-1])\n",
    "    \n",
    "    # [your code here] - find our max inputs and max outputs\n",
    "    max_inputs = ...\n",
    "    max_outputs = ...\n",
    "    \n",
    "    # [your code here] - calculate the number the model predicted correctly\n",
    "    num_correct = ...\n",
    "    \n",
    "    # [your code here] - calculate and return the accuracy\n",
    "    accuracy = ...\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to print out the accuracy of the model. Since we have a smaller sample size, it should be 100%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_accuracy(train_x, train_y, thetas)\n",
    "print(\"Accuracy: {0}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the exciting part of the project is having your chatbot make predictions, I'll be extra kind and give you a sneak preview of next week\n",
    "\n",
    "(I know I know this code is really ugly but I did it to try and deter people from trying to move too far ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What should I eat?\"\n",
    "random.choice(tag_responses.get(tags[np.argmax(sigmoid(forward_prop(np.array([build_bag(all_words, process_words(user_input))]), thetas, 1)[-1]))]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
