{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/15 Notebook - Customer Support Chatbot (Part B)\n",
    "\n",
    "Hello and welcome to this week's notebook! Last week, we learned how to create a custom data set, professionally clean data, and train a chat bot using a bag-of-words model. This week, we'll be making predictions from our model and creating a user interface for our chatbot\n",
    "\n",
    "The solutions to last week's notebook are incorporated into this notebook, so feel free to look through the methods to refresh yourself on how everything works. There will be a heading indicating where the new code begins\n",
    "\n",
    "There's nothing you need to change in the first part, but if you created a custom `intents.json`, make sure you replace that file here\n",
    "\n",
    "**Note: This notebook does NOT require any additional installation of `Keras` and `Tensorflow`. If you want to get some experience with these libaries, check out the other notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the methods you need to complete for the notebook:\n",
    "1. `predict_tag()`\n",
    "2. `get_response()`\n",
    "3. `chat()`\n",
    "\n",
    "**Note: You can skip reading many of the following cells, but make sure you run them so your model is trained**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing our libraries as always. Make sure you run the cell with `pip install nltk`, which will let you download the `nltk` library we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our nltk libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# install specific downloads\n",
    "nltk.download('punkt', quiet = True)\n",
    "nltk.download('wordnet', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other useful libraries (numpy == üêê)\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Modify your intents\n",
    "\n",
    "The great part about this chat bot is that it is fully customizable! Edit `intents.json` to your liking to create your own bot. Make sure that for each `intent`, you fill out the fields `tag`, `patterns`, and `responses`\n",
    "\n",
    "You can look at my file, `taco-bell-intents.json`, for reference\n",
    "\n",
    "Once you're done, you can continue to run the cells below!\n",
    "\n",
    "**Note: if you're having JSON formatting issues in the next cell, use [this link](https://jsonlint.com) to validate your JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"intents.json\").read()\n",
    "intents = json.loads(data_file)\n",
    "# when you print, you should see your JSON\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing the JSON\n",
    "\n",
    "We'll practice a common first step in any NLP project, data cleaning\n",
    "\n",
    "First, complete the function `process_words()` which will clean up our words according to the following steps:\n",
    "1. Get the tokens using `nltk.word_tokenize()`\n",
    "2. Set `cleaned_word` equal to the `lemmatized` and `lowercased` word\n",
    "\n",
    "**Note: Make sure you run the cell immediately below this first; it stores values needed in `process_words()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Set <code>tokens = nltk.word_tokenize(pattern)</code></li>\n",
    "    <li><code>lemmatizer.lemmatize(...)</code> will lemmatize a word</li>\n",
    "    <li>The paremeter of <code>lemmatizer.lemmatize(...)</code> should be <code>word.lower()</code></li>  \n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare needed variables for process_words()\n",
    "ignore_punctuation = [\"?\", \"!\", \".\", \",\"]\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(pattern):\n",
    "    # return variable\n",
    "    words = []\n",
    "    # get the tokens using nltk\n",
    "    tokens = nltk.word_tokenize(pattern)\n",
    "    for word in tokens:\n",
    "        # check if the word should be ignored\n",
    "        if word not in ignore_punctuation and word.isalnum():\n",
    "            # clean the word and add it to the list\n",
    "            cleaned_word = lemmatizer.lemmatize(word.lower())\n",
    "            words.append(cleaned_word)\n",
    "    # return the list\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "if (process_words(\"How was your day today?\") == ['how', 'wa', 'your', 'day', 'today']):\n",
    "    print(\"Nice work, sport!\")\n",
    "else:\n",
    "    print(\"Try again, buddy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `process_words()` to clean our words, we can parse the data from our JSON\n",
    "\n",
    "Complete the method `parse_intents()` which does the following:\n",
    "1. Set the value of `tag` from our `intent`\n",
    "2. Set `tokenized_words` using the helper method in `process_words()`\n",
    "3. Append a tuple of `tokenized_words` and `tag` to `tag_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Values of a JSON can be extracted using arrays</li>\n",
    "    <li>Let <code>tag = intent[\"tag\"]</code></li>\n",
    "    <li>Let <code>tokenized_words = process_words(pattern)</code></li>\n",
    "    <li>For the third step, the tuple can be appended with <code>tag_tokens.append((tokenized_words, tag))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_intents(intents):\n",
    "    # declare our needed variables\n",
    "    tags = []\n",
    "    all_words = []\n",
    "    tag_tokens = []\n",
    "    response_dict = dict()\n",
    "    \n",
    "    # iterate through each intent\n",
    "    for intent in intents[\"intents\"]:\n",
    "        \n",
    "        # add the noanswer tag to the dictionary (edge case)\n",
    "        if (intent[\"tag\"] == \"noanswer\"):\n",
    "            response_dict[\"noanswer\"] = intent[\"responses\"]\n",
    "        \n",
    "        # if the intent has no patterns, we can skip\n",
    "        if (len(intent[\"patterns\"]) == 0):\n",
    "            continue\n",
    "        \n",
    "        # add the tag to the list of tag\n",
    "        tag = intent[\"tag\"]\n",
    "        tags.append(tag)\n",
    "        \n",
    "        # update the dictionary\n",
    "        response_dict[tag] = intent[\"responses\"]\n",
    "        \n",
    "        # iterate through each pattern\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            # create our tokenized words\n",
    "            tokenized_words = process_words(pattern)\n",
    "            # add all the tokenized words to our words\n",
    "            all_words.extend(tokenized_words)\n",
    "            # adds a tuple -> (list of tokens, tag) -> to the list\n",
    "            tag_tokens.append((tokenized_words, tag))      \n",
    "    \n",
    "    # return our values in a tuple\n",
    "    return (np.array(tags), np.array(all_words), np.array(tag_tokens), response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this cool trick below to remove all duplicates from our arrays (and sort them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call our function\n",
    "tags, all_words, tag_tokens, tag_responses = parse_intents(intents)\n",
    "# sort and remove duplicates\n",
    "tags = np.array(sorted(list(set(tags))))\n",
    "all_words = np.array(sorted(list(set(all_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below and take a quick look to make sure that everything makes sense. It's hard for me to test your code without knowing what's in your JSON, but in general:\n",
    "\n",
    "- `tags` should contain a list of all your tags in the JSON, excluding `noanswer`\n",
    "- `all_words` should be a list of all the words in your JSON's patterns. There should be no duplicates or patterns that aren't words\n",
    "- Each entry of `tag_token_mappings` should have two values in a list. The first should be a list of patterns, and the second should be the tag of that pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tags: {0}\".format(tags))\n",
    "print(\"------\")\n",
    "print(\"All Words: {0}\".format(all_words))\n",
    "print(\"------\")\n",
    "print(\"Tag-Token Mappings: {0}\".format(tag_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a Training Set\n",
    "\n",
    "We know from previous lessons that the computer can't train a model without numeric values. To solve this, we'll use the `bag of words` technique we discussed in the Google Sheets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the helper method `build_bag()` which iterates through each `word` in `all_words`, and appends 1 to `bag` if the word is in `all_words`, and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The easiest way to do this is by using a simple <code>if else</code> statement</li>\n",
    "    <li>Recall that <code>A in B</code> will return <code>true</code> if the element A is in the iterable object B, and <code>false</code> otherwise</li>\n",
    "    <li>If you're feeling really fancy, you can just write <code>bag.append(1 * (word in tokens))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag(all_words, tokens):\n",
    "    # reset our current bag\n",
    "    bag = []\n",
    "    for word in all_words:\n",
    "        # add 0/1 if the word is in our token\n",
    "        in_token = (word in tokens)\n",
    "        bag.append(1 * in_token)\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "test_all_words = [\"edgar\", \"allen\", \"poe\", \"said\", \"the\", \"raven\", \"was\", \"nevermore\"]\n",
    "test_tokens = [\"quote\", \"the\", \"raven\", \"nevermore\"]\n",
    "if (build_bag(test_all_words, test_tokens) == [0, 0, 0, 0, 1, 1, 0, 1]):\n",
    "    print(\"You crushed it!\")\n",
    "else:\n",
    "    print(\"Ruh roh raggy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `build_training_set()` below, which performs the following steps:\n",
    "1. Grabs the value of `tokens`, the first (index 0) element of `tag_token`\n",
    "2. Grabs the value of `tag`, the second (index 1) element of `tag_token`\n",
    "3. Sets `current_bag` using the helper method `build_bag()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can get the values of <code>tokens</code> and <code>tag</code> with <code>tag_token[X]</code>, where <code>X</code> is 0 or 1, appropriately</li>\n",
    "    <li>Let <code>current_bag = build_bag(all_words, tokens)</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_set(tags, all_words, tag_tokens):\n",
    "    # define our variables to return\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "        \n",
    "    # iterate through each tag-token mapping\n",
    "    for tag_token in tag_tokens:\n",
    "        \n",
    "        # grab our needed values\n",
    "        tokens = tag_token[0]\n",
    "        tag = tag_token[1]\n",
    "        \n",
    "        # reset our current bag\n",
    "        current_bag = build_bag(all_words, tokens)\n",
    "            \n",
    "        # update our training inputs\n",
    "        train_x.append(current_bag)\n",
    "        \n",
    "        # set our outputs equal to 1 in the location\n",
    "        train_y.append(1 * (tags == tag))\n",
    "    \n",
    "    # return our values\n",
    "    return (np.array(train_x), np.array(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_training_set(tags, all_words, tag_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your `train_x` and `train_y` values in the following cell. It's hard for me to tell if you did everything correctly since you could be using a custom data set. If you have any questions about the program, feel free to message me on discord!\n",
    "\n",
    "- `train_x` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # words in `all_words`\n",
    "- `train_y` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # tags in `tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(\"Training Inputs: {0}\".format(train_x))\n",
    "print(\"-----\")\n",
    "print(\"Training Outputs: {0}\".format(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with training, you may notice that our data is very similarly grouped, specifically the training outputs. As you may have thought, this can cause some unwanted bias in our model. To fix this, we'll `shuffle` our training set by using `np.random.permutation()` and some clever array indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled indexes\n",
    "shuffled_indexes = np.random.permutation(train_x.shape[0])\n",
    "# set new values for train_x and train_y\n",
    "train_x = train_x[shuffled_indexes]\n",
    "train_y = train_y[shuffled_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Our Model from Scratch (no coding until the end)\n",
    "\n",
    "We have our cleaned, numeric inputs and outputs (`train_x` and `train_y`), so now what? \n",
    "\n",
    "It's time to train our model!\n",
    "\n",
    "**Note: In this version of the notebook, we'll be using the `numpy` neural network we developed last week. This neural network isn't as sophisticated as the one that Keras/Tensorflow generates, but you won't have to install any extra packages. If you want some experience working with other packages, check out the other version of the notebook (but know you will need to install Keras/Tensorflow)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will use the following architecture:\n",
    "\n",
    "<img src = \"./bag_of_words.PNG\" style=\"width:75%;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll copy and paste our nifty helper functions from last week, `sigmoid()` and `sigmoid_derivative()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # calculate the output of the sigmoid function and return it\n",
    "    sigmoid_val = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    # calculate the derivative and return the value\n",
    "    sigmoid_deriv = np.multiply(sigmoid(x), 1 - sigmoid(x))\n",
    "    return sigmoid_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our neural networks are broken down into two methods, defined below:\n",
    "1. `forward_prop()`\n",
    "2. `back_prop()`\n",
    "\n",
    "`forward_prop()` makes our predictions for a set of `thetas`, while `back_prop()` makes adjustments to these `thetas`\n",
    "\n",
    "We will combine these functions in `train_model()`, but first let's take a look at `forward_prop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(inputs, thetas, m):\n",
    "    # declare the values we need\n",
    "    outputs = []\n",
    "    curr_inputs = inputs\n",
    "    ones_col = np.ones((m, 1))\n",
    "    for theta in thetas:\n",
    "        # format the inputs by adding the column of ones\n",
    "        formatted_inputs = np.hstack((ones_col, curr_inputs))\n",
    "        # calculate the predicted value, and append it to the list of outputs\n",
    "        pred_val = formatted_inputs @ theta.T\n",
    "        outputs.append(pred_val)\n",
    "        # set curr_inputs to the the sigmoid of our predicted value\n",
    "        curr_inputs = sigmoid(pred_val)\n",
    "    # return our list of outputs\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define `back_prop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(y_predictions, y_actual, inputs, thetas, m, num_classifications):\n",
    "    # sets the constant for ones_col\n",
    "    ones_col = np.ones((m, 1))\n",
    "    \n",
    "    # calculates the \"difference\" for the final layer\n",
    "    diff3 = sigmoid(y_predictions[-1]) - y_actual\n",
    "    \n",
    "    # calculates the \"difference\" for the penultimate layer\n",
    "    diff2_unadjusted = diff3 @ thetas[1][:, 1:]\n",
    "    diff2 = np.multiply(diff2_unadjusted, sigmoid_derivative(y_predictions[0]))\n",
    "    \n",
    "    # formats the partial derivatives\n",
    "    format_partial_one = np.hstack((ones_col, np.asarray(inputs)))\n",
    "    format_partial_two = np.hstack((ones_col, np.asarray(sigmoid(y_predictions[0]))))\n",
    "    \n",
    "    # calculates the unadjusted partial derivatives\n",
    "    delta_one = diff2.T @ format_partial_one\n",
    "    delta_two = diff3.T @ format_partial_two\n",
    "    \n",
    "    # returns our partial derivatives divided by m (to scale)\n",
    "    return [delta_one / m, delta_two / m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll combine them in `train_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(thetas, inputs, actual_outputs, num_iterations, learning_rate, sample_size, num_classifications):\n",
    "    for iteration in range(num_iterations):\n",
    "        # calculate the outputs for the iteration\n",
    "        outputs = forward_prop(inputs, thetas, sample_size)\n",
    "        # calculate the gradients for the iteration\n",
    "        gradients = back_prop(outputs, actual_outputs, inputs, thetas, sample_size, num_classifications)\n",
    "        # adjust both of our thetas, taking a small step towards the minimum\n",
    "        thetas[0] = thetas[0] - learning_rate * gradients[0]\n",
    "        thetas[1] = thetas[1] - learning_rate * gradients[1]\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train our model, we need to define our `constants`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000\n",
    "learning_rate = 0.5\n",
    "sample_size = train_x.shape[0]\n",
    "num_classifications = train_y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as our `thetas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_nodes = 64\n",
    "theta_one = np.random.random((num_hidden_nodes, train_x.shape[1] + 1)) - 0.5\n",
    "theta_two = np.random.random((num_classifications, num_hidden_nodes + 1)) - 0.5\n",
    "thetas = [theta_one, theta_two]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's finally time to `train` our model! ü§û"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = train_model(thetas, train_x, train_y, num_iterations, learning_rate, sample_size, num_classifications)\n",
    "print(thetas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to make the chatbot GUI, test the model, and other fun stuff (unfortunately next week üò¢), we need to test the `accuracy` of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the formula for `accuracy`:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{200}&space;accuracy&space;=&space;\\frac{n_{correct}}{n_{total}}\" title=\"accuracy = \\frac{n_{correct}}{n_{total}}\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function `test_accuracy()` completes the following steps:\n",
    "\n",
    "1. Calculate `max_inputs` and `max_outputs` by using `np.argmax()` and setting the `axis` parameter equal to 1\n",
    "2. Calculate `num_correct`\n",
    "3. Calculate `accuracy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use <code>np.argmax(A, axis = 1)</code> for <code>max_inputs</code> and <code>max_outputs</code>, where <code>A</code> is the array you want to find the max of</li>\n",
    "    <li><code>num_correct</code> can be found by using <code>np.sum()</code> with the values where <code>max_inputs == max_outputs</code></li>\n",
    "    <li>Use the formula to calculate <code>accuracy</code>!</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(x, y, thetas):\n",
    "    # get our outputs by forward propogating\n",
    "    outputs = sigmoid(forward_prop(x, thetas, x.shape[0])[-1])\n",
    "    \n",
    "    # find our max inputs and max outputs\n",
    "    max_inputs = np.argmax(outputs, axis = 1)\n",
    "    max_outputs = np.argmax(y, axis = 1)\n",
    "    \n",
    "    # calculate the number the model predicted correctly\n",
    "    num_correct = np.sum(max_inputs == max_outputs)\n",
    "    \n",
    "    # calculate and return the accuracy\n",
    "    accuracy = num_correct / max_inputs.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to print out the accuracy of the model. Since we have a smaller sample size, it should be 100%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_accuracy(train_x, train_y, thetas)\n",
    "print(\"Accuracy: {0}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Finishing the Chatbot\n",
    "\n",
    "Now that we have everything we need, we can finish the chatbot!\n",
    "\n",
    "As a reminder, these are the methods you need to complete:\n",
    "1. `predict_tag()`\n",
    "2. `get_response()`\n",
    "3. `chat()`\n",
    "\n",
    "Again, this is quite a short notebook, so feel free to spend some time looking over and finishing the first part if you need to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Predicting the Tag\n",
    "\n",
    "Last week we created a neural network that took our training data as an input, and matched it to a tag output. Now, we need to predict tags using custom user inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `predict_tag()`, which does the following\n",
    "1. Sets `process_input` to the cleaned and tokenized `user_input`\n",
    "2. Sets `bag_input` to the bag representation of `process_input`\n",
    "3. Calculates `pred_tag_values` as the `sigmoid` of the last array output in `layer_outputs`\n",
    "4. Finds the values for `max_value_tag` and `probability`, the maximum index and value, respectively, of `pred_tag_values`\n",
    "5. Gets the value of `pred_tag` using `max_value_tag` and the list of `tags`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>process_words()</code> helper function with <code>user_input</code> as the parameter</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>build_bag()</code> helper function with <code>all_words</code> and <code>process_input</code> as the parameters</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Using index <code>[-1]</code> will get the last element of any numpy array</li>\n",
    "    <li>Call the <code>sigmoid()</code> helper function on this last element to set <code>pred_tag_values</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>np.argmax()</code> will return the index of the maximum value of a numpy array. <a href = https://numpy.org/doc/stable/reference/generated/numpy.argmax.html>Documentation</a></li>\n",
    "    <li><code>np.max()</code> will return the maximum value of a numpy array. <a href = https://numpy.org/doc/stable/reference/generated/numpy.amax.html>Documentation</a></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 5</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>pred_tag</code> will be the value of the numpy array <code>tags</code> at index <code>max_value_tag</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tag(user_input, thetas):\n",
    "    # [your code here] - tokenize/clean inputs\n",
    "    process_input = ...\n",
    "    \n",
    "    # [your code here] - build the bag\n",
    "    bag_input = ...\n",
    "    \n",
    "    # forward propogate our values\n",
    "    layer_outputs = forward_prop(np.array([bag_input]), thetas, 1)\n",
    "    \n",
    "    # [your code here] - get the final output layer and take the sigmoid\n",
    "    pred_tag_values = ...\n",
    "    \n",
    "    # [your code here] - get the index and value of the largest probability value\n",
    "    max_value_tag = ...\n",
    "    probability = ...\n",
    "    \n",
    "    # [your code here] - predict the tag and return\n",
    "    pred_tag = ...\n",
    "    return (pred_tag, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try your own inputs here and make sure that the tag makes sense!\n",
    "# look at the probability for the bot's confidence level\n",
    "custom_input = \"How are you today?\"\n",
    "predict_tag(custom_input, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Getting a Response\n",
    "\n",
    "Now that we have a way to predict our tags, we need to get a user input from this tag "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `get_response()`, which does the following\n",
    "1. Sets `pred_tag` and `probability` using our helper method `predict_tag()`\n",
    "2. Gets the correct responses from the `tag_responses` dictionary. If the `probability` is not high enough, it should equal `tag_responses[\"noanswer\"]`\n",
    "3. Sets the boolean value of `should_exit_bot`, which should be `true` if the predicted tag is `\"goodbye\"` or `\"thanks\"`\n",
    "4. Get a random `response` from our `responses`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>predict_tag()</code> helper function with <code>user_input</code> and <code>thetas</code> as the parameters</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Write your code in the format <code>responses = [_____] if [_____] else [_____]</code></li>\n",
    "    <li>The first blank should be the value from the dictionary, <code>tag_responses[pred_tag]</code></li>\n",
    "    <li>The second blank should be the condition when <code>probability > error_margin</code></li>\n",
    "    <li>The final blank should be the invalid response tag, <code>tag_responses[\"noanswer\"]</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The value should be <code>true</code> with the goodbye tag, so set <code>should_exit_bot</code> equal to <code>pred_tag == \"goodbye\" or pred_tag == \"thanks\"</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>np.random.choice()</code> will return a random element from an array. Read more about it <a href = https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html>here</a></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input, thetas, error_margin):\n",
    "    # [your code here] - get the predicted tag and probability\n",
    "    pred_tag, probability = ...\n",
    "    \n",
    "    # [your code here] - get a list of different responses\n",
    "    responses = ...\n",
    "    \n",
    "    # [your code here] - check if we should exit the bot\n",
    "    should_exit_bot = ...\n",
    "    \n",
    "    # [your code here] - get the response\n",
    "    response = ...\n",
    "    \n",
    "    # return the variables\n",
    "    return (response, should_exit_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try your own inputs here and check the responses! (remember the second output should only be true for goodbye messages)\n",
    "custom_input = \"How are you today?\"\n",
    "get_response(custom_input, thetas, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating the Chat User Interface\n",
    "\n",
    "Everything is set up! We just need to put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `chat()`, which puts it all together:\n",
    "1. Sets `user_input` using Python's `input()` function. I recommend putting in `human_prefix` as the parameter. Read more about it [here](https://www.w3schools.com/python/ref_func_input.asp)\n",
    "2. Finds `response` and `should_exit` using our helper method `get_response`\n",
    "3. Prints the `response` preceded by the `robot_prefix`\n",
    "4. Sets `continue_chat` appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>If you set <code>user_input</code> equal to <code>input()</code>, it will automatically store the user's input</li>\n",
    "    <li>Set the parameter of <code>input()</code> to be <code>human_prefix</code> to improve your UI</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Call <code>get_response()</code> with <code>user_input</code>, <code>thetas</code> and <code>0.25</code> as the parameters. The last value can be anything, depending on how accurate you want your bot to be</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use string concatenation to combine the strings</li>\n",
    "    <li>Print <code>robot_prefix + response</code> to the console</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>We should continue when we shouldn't exit, so set <code>continue_chat = not should_exit</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    # initialize variables\n",
    "    continue_chat = True\n",
    "    robot_prefix = \"Bot: \"\n",
    "    human_prefix = \"You: \"\n",
    "    \n",
    "    # give an introduction\n",
    "    print(robot_prefix + \"Hi! I am a bot offering support for Taco Bell. Ask me what I can do! To exit, say goodbye\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # continue while the user doesn't say goodbye\n",
    "    while (continue_chat):\n",
    "        # [your code here] - get the user input from the console\n",
    "        user_input = ...\n",
    "        \n",
    "        # [your code here] - get the response and exit condition from the helper function\n",
    "        response, should_exit = ...\n",
    "        \n",
    "        # [your code here] - print the bot's response\n",
    "        print(...)\n",
    "        print(\"\")\n",
    "        \n",
    "        # [your code here] - set the exit condition\n",
    "        continue_chat = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's everything you need! If you made it this far, congratulations, you just made your first chat bot! Run the method below to test it out\n",
    "\n",
    "If you want to add more patterns/responses to the bot, you can modify `intents.json` to your liking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
