{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/1: Neural Networks\n",
    "\n",
    "Hi everyone! In this notebook we'll be looking at Neural Networks and their applications in Machine Learning and Natural Language Processing\n",
    "\n",
    "To complete this notebook, we have the following methods for you to complete:\n",
    "\n",
    "1. `sigmoid()`\n",
    "2. `sigmoid_derivative()`\n",
    "\n",
    "\n",
    "We'll start by building neural networks to decide if two boolean inputs are equivalent, and then extend it to recognizing handwritten digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sigmoid and the Derivative of Sigmoid\n",
    "\n",
    "The sigmoid function and its derivative are essential for the functionality of neural networks. To start, complete the function `sigmoid()`\n",
    "\n",
    "Recall from a couple of weeks ago that the formula for `sigmoid()` is:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{150}&space;\\large&space;\\frac{1}{1&space;&plus;&space;e^{-x}}\" title=\"\\large \\frac{1}{1 + e^{-x}}\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # calculate the output of the sigmoid function and return it\n",
    "    sigmoid_val = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct value for sigmoid!\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test sigmoid\n",
    "if (sigmoid(0) == 0.5 and sigmoid(3.1415) == 0.9585724885979936):\n",
    "    print(\"Correct value for sigmoid!\")\n",
    "else:\n",
    "    print(\"I think you gotta fix that pal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll calculate the derivative of `sigmoid()` in the function `sigmoid_derivative()`. The derivative of `sigmoid()` is:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{150}&space;\\large&space;\\newline&space;=&space;\\frac{d}{dx}\\frac{1}{1&space;&plus;&space;e^{-x}}&space;\\newline&space;\\newline&space;=&space;-1&space;(1&space;&plus;&space;e^{-x})^{-2}&space;\\newline&space;\\newline&space;=&space;sigmoid(x)&space;*&space;(1&space;-&space;sigmoid(x))\" title=\"\\large \\newline = \\frac{d}{dx}\\frac{1}{1 + e^{-x}} \\newline \\newline = -1 (1 + e^{-x})^{-2} \\newline \\newline = sigmoid(x) * (1 - sigmoid(x))\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    # calculate the derivative and return the value\n",
    "    sigmoid_deriv = np.multiply(sigmoid(x), 1 - sigmoid(x))\n",
    "    return sigmoid_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "# run this cell to check your derivative of sigmoid\n",
    "if (sigmoid_derivative(0) == 0.25 and sigmoid_derivative(3.1415) == 0.03971127270104303):\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"bad bad job >:(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating our Neural Network Methods\n",
    "\n",
    "Our neural network system depends on two main functions:\n",
    "1. `Forward Propogation`\n",
    "2. `Back Propogation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by implementing `forward_prop()` for our neural network\n",
    "\n",
    "You will need to complete the following steps in the method:\n",
    "\n",
    "1. Set `ones_col` equal to the correctly-sized matrix of ones\n",
    "2. Calculate `pred_val` from `formatted_inputs` and `theta`\n",
    "3. Set `curr_inputs` to the `sigmoid()` of `pred_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(inputs, thetas, m):\n",
    "    # declare the values we need\n",
    "    outputs = []\n",
    "    curr_inputs = inputs\n",
    "    ones_col = np.ones((m, 1))\n",
    "    for theta in thetas:\n",
    "        # format the inputs by adding the column of ones\n",
    "        formatted_inputs = np.hstack((ones_col, curr_inputs))\n",
    "        # calculate the predicted value, and append it to the list of outputs\n",
    "        pred_val = formatted_inputs @ theta.T\n",
    "        outputs.append(pred_val)\n",
    "        # set curr_inputs to the the sigmoid of our predicted value\n",
    "        curr_inputs = sigmoid(pred_val)\n",
    "    # return our list of outputs\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You did it, champ!\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test your implementation of forward_prop\n",
    "np.random.seed(123456789)\n",
    "test_thetas = [np.random.random((2, 3)), np.random.random((1, 3))]\n",
    "test_inputs = np.random.random((4, 2))\n",
    "\n",
    "test_forward_prop = forward_prop(test_inputs, test_thetas, 4)\n",
    "correct_forward_prop_val = [np.array([[1.24524706, 1.42359831], [1.34984787, 1.52210102], [0.58694045, 0.74140142], [0.66236153, 0.78210622]]), np.array([[1.54454925], [1.55730176], [1.43758967], [1.44616478]])]\n",
    "\n",
    "if not (test_forward_prop[0] - correct_forward_prop_val[0]).all():\n",
    "    print(\"Your first layer values are incorrect, sport\")\n",
    "elif not (test_forward_prop[1] - correct_forward_prop_val[1]).all():\n",
    "    print(\"Your second layer values are incorrect, ace\")\n",
    "else:\n",
    "    print(\"You did it, champ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method, `back_prop()` is an absolute banger, but easily the most important for neural networks to function. This method will take the derivative of each layer and then compute the gradients for each set of thetas\n",
    "\n",
    "You will need to complete the following steps:\n",
    "1. Calculate `diff3`, the difference of the `sigmoid()` of the last entry of `y_predictions` and `y_actual`\n",
    "2. Calculate `diff2`, the multiplication of `diff2_unadjusted` and the `sigmoid_derivative()` of the first entry of \n",
    "`y_predictions`\n",
    "3. Calulate `delta_one`, the matrix multiplication of `diff2` and `format_partial_one` (Note the dimensions!)\n",
    "4. Calulate `delta_two`, the matrix multiplication of `diff3` and `format_partial_two` (Note the dimensions!)\n",
    "\n",
    "Note: Unlike `forward_prop()`, this method is very difficult to code so that it works for any-sized neural network. Hence, we will hard code it with the assumption that there are 2 layers for our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(y_predictions, y_actual, inputs, thetas, m, num_classifications):\n",
    "    # sets the constant for ones_col\n",
    "    ones_col = np.ones((m, 1))\n",
    "    \n",
    "    # adjusts the value of actual_val if there are more than 2 classifications\n",
    "    if (num_classifications > 2):\n",
    "        y_actual = (np.eye(num_classifications))[actual_val]\n",
    "    \n",
    "    # calculates the \"difference\" for the final layer\n",
    "    diff3 = sigmoid(y_predictions[-1]) - y_actual\n",
    "    \n",
    "    # calculates the \"difference\" for the penultimate layer\n",
    "    diff2_unadjusted = diff3 @ thetas[1][:, 1:]\n",
    "    diff2 = np.multiply(diff2_unadjusted, sigmoid_derivative(y_predictions[0]))\n",
    "    \n",
    "    # formats the partial derivatives\n",
    "    format_partial_one = np.hstack((ones_col, np.asarray(inputs)))\n",
    "    format_partial_two = np.hstack((ones_col, np.asarray(sigmoid(y_predictions[0]))))\n",
    "    \n",
    "    # calculates the unadjusted partial derivatives\n",
    "    delta_one = diff2.T @ format_partial_one\n",
    "    delta_two = diff3.T @ format_partial_two\n",
    "    \n",
    "    # returns our partial derivatives\n",
    "    return [delta_one / m, delta_two / m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't believe you actually did it you're insane!\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test back_prop\n",
    "np.random.seed(123456789)\n",
    "\n",
    "test_thetas = [np.random.random((2, 3)), np.random.random((1, 3))]\n",
    "test_inputs = np.random.random((4, 2))\n",
    "test_y_actual = np.random.random((4, 1))\n",
    "\n",
    "test_back_prop = back_prop(test_forward_prop, test_y_actual, test_inputs, test_thetas, 4, 2)\n",
    "correct_back_prop = [np.array([[0.00907003, 0.0054814 , 0.00563069], [0.03581553, 0.02146392, 0.021947  ]]), np.array([[0.33095398, 0.25353556, 0.26271239]])]\n",
    "\n",
    "if not (test_back_prop[0] - correct_back_prop[0]).any():\n",
    "    print(\"Homie, your first partial derivative is wrong\")\n",
    "elif not (test_back_prop[1] - correct_back_prop[1]).any():\n",
    "    print(\"Oh no no your second partial derivative is wrong\")\n",
    "else:\n",
    "    print(\"I can't believe you actually did it you're insane!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using our Neural Network Architecture\n",
    "\n",
    "### Note: You don't have to code for the rest of the notebook! You can relax and watch your hard work pay off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the two needed functions for neural networks, we can start predicting stuff!\n",
    "\n",
    "We'll start by predicting the `xnor` operator, which essentially checks if two boolean inputs are equivalent\n",
    "\n",
    "We can use the following truth table to describe `xnor`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| a | b | out |\n",
    "| - | - | --- |\n",
    "| 1 | 1 |  1  |\n",
    "| 1 | 0 |  0  |\n",
    "| 0 | 1 |  0  |\n",
    "| 0 | 0 |  1  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the picture below describes our neural network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define our inputs and outputs, as well as our values for theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining inputs and outputs\n",
    "xnor_inputs = np.array([[1, 1, 0, 0], [1, 0, 1, 0]]).T\n",
    "xnor_outputs = np.array([[1, 0, 0, 1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining theta values with correct dimensions\n",
    "xnor_thetas = [np.random.random((2, 3)), np.random.random((1, 3))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll define constants for our gradient descent algorithm\n",
    "\n",
    "You might notice that `learning_rate` $ > 1$ Why?\n",
    "\n",
    "This is undoubtably because I messed up my code somewhere but I have no idea why this happens all I know is that this is the value that makes gradient descent work\n",
    "\n",
    "It probably has to do with the fact that the sample size is so small, so we need to converge much quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining constants\n",
    "sample_size = xnor_inputs.shape[0]\n",
    "num_classifications = 2\n",
    "learning_rate = 5\n",
    "num_iterations = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run gradient descent for our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "for iteration in range(num_iterations):\n",
    "    # calculate the outputs for the iteration\n",
    "    outputs = forward_prop(xnor_inputs, xnor_thetas, sample_size)\n",
    "    # calculate the gradients for the iteration\n",
    "    gradients = back_prop(outputs, xnor_outputs, xnor_inputs, xnor_thetas, sample_size, num_classifications)\n",
    "    # adjust both of our thetas, taking a small step towards the minimum\n",
    "    xnor_thetas[0] = xnor_thetas[0] - learning_rate * gradients[0]\n",
    "    xnor_thetas[1] = xnor_thetas[1] - learning_rate * gradients[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you'll see that the outputs read\n",
    "\n",
    "$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix}$\n",
    "\n",
    "when you run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "[[1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 0]]\n",
      "--------\n",
      "Outputs (rounded):\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# run this cell to check your code!\n",
    "print(\"Inputs:\")\n",
    "print(str(xnor_inputs))\n",
    "print(\"--------\")\n",
    "print(\"Outputs (rounded):\")\n",
    "print(str(np.round(sigmoid(outputs[-1]), 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
