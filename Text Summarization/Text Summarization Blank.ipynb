{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/26 Notebook - Text Summarization\n",
    "\n",
    "Hi everyone! This week, we'll be taking a look at `text summarization`, basically how we can use technology to extract important information from a long piece of text. We'll also take a look at `web scraping` to extract any Wikipedia article, a useful technique you can use for a lot of your personal projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Learn the basics of web scraping\n",
    "- Become more familiar with `nltk` and `numpy` libraries\n",
    "- Create a program to summarize Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this notebook, you'll have to compelete the following methods:\n",
    "1. `format_paragraphs()`\n",
    "2. `build_freq_dict()`\n",
    "3. `build_ratio_dict()`\n",
    "4. `calc_sentence_weight()`\n",
    "5. `create_sentence_weights()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web Scraping and Cleaning Our Data\n",
    "\n",
    "Since we're not using a predefined data set, we're going to need to make our own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by installing `BeautifulSoup4`, a library commonly used for formatting and web scraping in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import our needed libraries for scraping the Wikipedia article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below will do all of the web scraping. It takes in a single parameter, `wiki_url`, the URL of the Wikipedia article, which you'll be able to change in the next cell. We first use `urllib` to open the HTML of the webpage, and then use `Beautiful Soup` to parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(wiki_url):\n",
    "    # sends a urlopen request to the wikipedia article and reads the html\n",
    "    scraped_data = urllib.request.urlopen(wiki_url)\n",
    "    article = scraped_data.read()\n",
    "\n",
    "    # gets all of the HTML paragraphs from the Wikipedia article\n",
    "    return bs.BeautifulSoup(article, \"lxml\").find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to change the URL!\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "wiki_paragraphs = get_data(wiki_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `format_paragraphs()` below. Most of the method is filled out for you, but you need to loops through each paragraph in `paragraphs`, and append its `.text` value to `text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Loop through each paragraph by writing <code>for paragraph in paragraphs</code></li>\n",
    "    <li>Append to <code>text</code> by adding the value of <code>paragraph.text</code></li>\n",
    "</ul>\n",
    "    <details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <code>def format_paragraphs(paragraphs):\n",
    "    text = \"\"\n",
    "    # append all the paragraphs\n",
    "    for paragraph in paragraphs:\n",
    "        text += paragraph.text\n",
    "    # return the cleaned text\n",
    "    return clean_text(text)</code>\n",
    "</ul>\n",
    "</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_paragraphs(paragraphs):\n",
    "    text = \"\"\n",
    "    # [your code here] - append all the paragraphs\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    # return the cleaned text\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # get rid of useless characters (references and extra spaces)\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # return cleaned text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to get the formatted article! (print it if you like)\n",
    "wiki_article = format_paragraphs(wiki_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating a Model\n",
    "\n",
    "The second step in our pipeline is create the model for this project, a table of weighted frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using `nltk`, the natural language toolkit library, to tokenize our text throughout this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin tokenizing, let's store our `stopwords`. If you're feeling **spicy** you can switch the language to something else and find a wikipedia article in a different language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `build_freq_dict()`, which does the following:\n",
    "1. Tokenizes the words of the articles using `tokenizer.tokenize()`. You can read more about the method [here](https://docs.python.org/3/library/tokenize.html)\n",
    "2. Finds the appropriate `frequency` if it's in the `freqs` dictionary, or 0 if it's not in the `freqs` dictionary\n",
    "3. Increments `frequency` by 1\n",
    "4. Updates `freqs` with the new `frequency`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>This can be done by setting <code>tokenized_words</code> equal to <code>tokenizer.tokenize(article)</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can use the dictionary <code>get()</code> function using the <code>default</code> parameter. The documentation can be found <a href = \"https://www.tutorialspoint.com/python/dictionary_get.htm\">here</a>.</li>\n",
    "    <li>Call <code>freqs.get()</code> with parameters <code>word</code> and <code>0</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>frequency</code> can be incremented with the <code>+=</code> oeprator</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The <code>frequency</code> can be updated by writing <code>freqs[word] = frequency</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq_dict(article):\n",
    "    # initialize the dictionary\n",
    "    freqs = dict()\n",
    "    \n",
    "    # creates the tokenizer\n",
    "    article = article.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # [your code here] - tokenize the word\n",
    "    tokenized_words = ...\n",
    "    \n",
    "    # iterates through each token\n",
    "    for word in tokenized_words:\n",
    "        # skips stopwords\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        \n",
    "        # [your code here] - gets the frequency\n",
    "        frequency = ...\n",
    "        \n",
    "        # [your code here] - increments the frequency by 1\n",
    "        frequency += ...\n",
    "        \n",
    "        # [your code here] - updates the frequency\n",
    "        ...\n",
    "    \n",
    "    # return the dictionary\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your method\n",
    "test_article = \"\"\"In the night, I hear them talk the coldest story ever told\n",
    "                Somewhere far along this road, he lost his soul\n",
    "                To a woman so heartless\n",
    "                How could you be so heartless\n",
    "                Oh, how could you be so heartless?\"\"\"\n",
    "\n",
    "test_dict = build_freq_dict(test_article)\n",
    "\n",
    "actual_dict = {'night': 1, 'hear': 1, 'talk': 1, 'coldest': 1, 'story': 1, 'ever': 1, 'told': 1,\n",
    "               'somewhere': 1, 'far': 1, 'along': 1, 'road': 1, 'lost': 1, 'soul': 1, 'woman': 1,\n",
    "               'heartless': 3, 'could': 2, 'oh': 1}\n",
    "\n",
    "if (test_dict != actual_dict):\n",
    "    print(\"Sorry homeboy/homegirl\")\n",
    "else:\n",
    "    print(\"Zoo Wee Mama!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you passed the test case above, you can run the cell below to store the frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = build_freq_dict(wiki_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our frequency dictionary, we need to calculate the weighted frequency of occurrence\n",
    "\n",
    "We use the following formula to obtain the correct value:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{300}&space;ratio_i&space;=&space;\\frac{frequency_i}{max_{frequency}}\" title=\"ratio_i = \\frac{frequency_i}{max_{frequency}}\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `build_ratio_dict()`, which converts our dictionary of frequencies into a dictionary of occurence weights as follows:\n",
    "\n",
    "1. Set `frequencies` equal to a `numpy array` of the `values()` of `freqs`\n",
    "2. Finds the maximum frequency in `frequencies`\n",
    "3. Calculates `ratios` according to the formula above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You're going to need to first cast the values to a <code>list</code>, and then a <code>numpy array</code></li>\n",
    "</ul>\n",
    "    <details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <code>frequencies = np.array(list(freqs.values()))</code>\n",
    "</ul>\n",
    "</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use <code>max()</code> on <code>freqs.values()</code> to find the <code>max_frequency</code></li>\n",
    "</ul>  \n",
    "<p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the formula above to calculate the ratio, replacing the numerator with <code>frequencies</code></li>\n",
    "</ul>  \n",
    "<p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ratio_dict(freqs):\n",
    "    # [your code here] - gets the frequencies\n",
    "    frequencies = ...\n",
    "    \n",
    "    # [your code here] - finds the maximum frequency\n",
    "    max_frequency = ...\n",
    "    \n",
    "    # [your code here] - calculates the ratios\n",
    "    ratios = ...\n",
    "    \n",
    "    # returns the appropriate dictionary\n",
    "    return dict(zip(freqs.keys(), ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your code\n",
    "test_ratio_dict = build_ratio_dict(test_dict)\n",
    "\n",
    "actual_dict =  {'night': 0.3333333333333333, 'hear': 0.3333333333333333, 'talk': 0.3333333333333333, \n",
    "                'coldest': 0.3333333333333333, 'story': 0.3333333333333333, 'ever': 0.3333333333333333, \n",
    "                'told': 0.3333333333333333, 'somewhere': 0.3333333333333333, 'far': 0.3333333333333333, \n",
    "                'along': 0.3333333333333333, 'road': 0.3333333333333333, 'lost': 0.3333333333333333, \n",
    "                'soul': 0.3333333333333333, 'woman': 0.3333333333333333, 'heartless': 1.0, \n",
    "                'could': 0.6666666666666666, 'oh': 0.3333333333333333}\n",
    "\n",
    "if (actual_dict != test_ratio_dict):\n",
    "    print(\"Awful, gross, yuck!\")\n",
    "else:\n",
    "    print(\"aw yea B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your function is working, run the cell below to store your dictionary of weighted occurence ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_dict = build_ratio_dict(freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Calculating Sentence Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the weight of each word, we can find the weight of the entire sentences in the article. The sentences with a higher weight will be the best at summarizing our text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by taking our original article, tokenize it by sentences, and storing it in `wiki_sentences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = nltk.sent_tokenize(wiki_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also initialize some constants to use for factoring in the length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_limit = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `calc_sentence_weight()`, which calculates the sentence weight as follows:\n",
    "1. Set `tokenized_words` using the `tokenizer.tokenize()` library function\n",
    "2. Get the `weight` from `ratio_dict`. If the current `word` is not in the dictionary, set `weight` equal to 0\n",
    "3. Add the value of `weight` to `total_weight`\n",
    "\n",
    "*Note: At the end we factor in the total length of the sentence to prioritize shorter sentences*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use <code>tokenizer.tokenize()</code> with <code>sentence</code> as the parameter</li>\n",
    "</ul>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can use the dictionary <code>get()</code> function using the <code>default</code> parameter. The documentation can be found <a href = \"https://www.tutorialspoint.com/python/dictionary_get.htm\">here</a>.</li>\n",
    "    <li>Call <code>freqs.get()</code> with parameters <code>word</code> and <code>0</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>+=</code> operator with <code>weight</code> to increment the total weight</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sentence_weight(sentence, ratio_dict):\n",
    "    # intiializes the total weight to 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    # creates the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # [your code here] - tokenizes the word\n",
    "    tokenized_words = ...\n",
    "    \n",
    "    # edge cases\n",
    "    if (len(tokenized_words) == 0 or len(tokenized_words) >= length_limit):\n",
    "        return 0\n",
    "    \n",
    "    # iterates through each token\n",
    "    for word in tokenized_words:\n",
    "        \n",
    "        # skips stopwords\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        \n",
    "        # [your code here] - finds the weight in the dictionary, 0 if it doesn't exist\n",
    "        weight = ...\n",
    "        \n",
    "        # [your code here] - adds this value to the total weight\n",
    "        total_weight += ...\n",
    "    \n",
    "    # return the total weight\n",
    "    return total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your function\n",
    "test_sentence = \"In the night I hear them talk\"\n",
    "test_weight = calc_sentence_weight(test_sentence, test_ratio_dict)\n",
    "\n",
    "if (test_weight != 1):\n",
    "    print(\"Looks like something went wrong, Buster\")\n",
    "else:\n",
    "    print(\"~:)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our helper function is working, we can create a list of weights, one for each sentence in our Wikipedia article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function `create_sentence_weights()`, which uses `calc_sentence_weight()` to make a list of sentence weights as follows:\n",
    "\n",
    "1. Iterates through each `sentence` in `sentences`\n",
    "2. Calculates the `weight` using `calc_sentence_weight()`\n",
    "3. Appends `weight` to the list of `weights` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Iterate through each <code>sentence</code> with <code>for sentence in sentences:</code></li>\n",
    "</ul>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Call <code>calc_sentence_weight()</code> with <code>sentence</code> and <code>ratio_dict</code> as the parameters</li>\n",
    "</ul>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use <code>.append()</code> on <code>weights</code> to add the <code>weight</code>. You can find the documentation <a href = \"https://www.w3schools.com/python/ref_list_append.asp\">here</a>.</li>\n",
    "</ul>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_weights(sentences, ratio_dict):\n",
    "    # initialize the list\n",
    "    weights = []\n",
    "    \n",
    "    # [your code here] - iterate through each sentence\n",
    "    ...\n",
    "        # [your code here] - calculate the weight and add it to the list\n",
    "        weight = ...\n",
    "        weights.append(...)\n",
    "        \n",
    "    # return the weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test your method\n",
    "test_sentences = [\"Because I'm heartless\", \"It's the coldest outside this night!\"]\n",
    "test_weights = create_sentence_weights(test_sentences, test_ratio_dict)\n",
    "\n",
    "if (test_weights != [1.0, 0.6666666666666666]):\n",
    "    print(\"Your method is kinda smelly rn ngl\")\n",
    "else:\n",
    "    print(\"Hot diggity dog!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get a list of weights for our article!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weights = create_sentence_weights(wiki_sentences, ratio_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Putting Together a Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put the summary together, we're going to grab the `num_sentences` highest sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitialize our values\n",
    "num_sentences = 5\n",
    "wiki_weights = np.array(wiki_weights)\n",
    "wiki_sentences = np.array(wiki_sentences)\n",
    "# gets the \"num_sentences\" highest scores\n",
    "largest_weight_indexes = wiki_weights.argsort()[-num_sentences:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll concatenate all of these sentences to make a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorts them \"chronologically\", so that it makes more sense\n",
    "largest_weight_indexes.sort()\n",
    "# retrieves the highest-scoring sentences\n",
    "summary_sentences = wiki_sentences[largest_weight_indexes]\n",
    "# joins the sentences\n",
    "summary = \" \".join(summary_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can print our summary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the summary makes sense! More specifically, it should be a general overview of the wikipedia article that you chose. I have noticed that this model does struggle with celebrities, probably because the weights need to be adjusted specifically for the name of the celebrity. Either way, congratulations! You just created a program to summarize a large portion of text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Visualization (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do some simple visualizations to see the weight of our words in our text\n",
    "\n",
    "**Note: I will be dicussing the data in the Natural Language Processing Wikipedia page, which can be found [here](https://en.wikipedia.org/wiki/Natural_language_processing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by installing and importing two libraries, `colored` and `colour`, used for color visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install colored -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install colour -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colored\n",
    "from colour import Color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a gradient of colors to represent the strength of our colors with the `colour` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = Color(\"#f0584f\")\n",
    "white = Color(\"#f7e5e4\")\n",
    "gradient = list(white.range_to(red, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a function, `make_colored_text()`, that will take in text and output a colored version of the text. Feel free to look more in depth into it to see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_colored_text(text, gradient, weights):\n",
    "    # declare the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # create a list of words\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    # initialize our colored summary\n",
    "    colored_summary = colored.bg(gradient[0].hex)\n",
    "    \n",
    "    # iterate through each word\n",
    "    for word in words:\n",
    "        # tokenize/clean the word\n",
    "        word_formatted = tokenizer.tokenize(word.lower())\n",
    "        \n",
    "        # edge case\n",
    "        if (len(word_formatted) == 0):\n",
    "            continue\n",
    "            \n",
    "        # access appropriate word\n",
    "        word_formatted = tokenizer.tokenize(word.lower())[0]\n",
    "        \n",
    "        # grab the appropriate gradient index\n",
    "        gradient_index = int((len(gradient) - 1) * (weights.get(word_formatted, 0)))\n",
    "        \n",
    "        # color the word and append it to the summary using our libraries\n",
    "        colored_word = colored.bg(gradient[gradient_index].hex) + word\n",
    "        colored_summary += colored_word + colored.bg(gradient[0].hex) + \" \"\n",
    "    \n",
    "    # return the coloerd summary\n",
    "    return colored_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and visualize our entire article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colored_text = make_colored_text(wiki_article, gradient, ratio_dict)\n",
    "print(colored_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The darker the color is, the more significant the word is. You'll notice that words such as `natural`, `language`, `processing`, and `algorithms` are darker because they appear the most frequently in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we color our summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_summary = make_colored_text(summary, gradient, ratio_dict)\n",
    "print(colored_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that overall, our words are more red in color compared to the article. This makes perfect sense! We're trying to find the most relevant information, so our summary should be more colorful overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
