{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/26 Notebook - Text Summarization\n",
    "\n",
    "Hi everyone! This week, we'll be taking a look at `text summarization`, basically how we can use technology to extract important information from a long piece of text. We'll also take a look at `web scraping` to extract any Wikipedia article, a useful technique you can use for a lot of your personal projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Learn the basics of web scraping\n",
    "- Become more familiar with `nltk` and `numpy` libaries\n",
    "- Create a program to summarize Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this notebook, you'll have to compelete the following methods:\n",
    "1. `format_paragraphs()`\n",
    "2. `asd`\n",
    "3. `asd`\n",
    "4. `asd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web Scraping and Cleaning Our Data\n",
    "\n",
    "Since we're not using a predefined data set, we're going to need to make our own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by installing `BeautifulSoup4`, a library commonly used for formatting and web scraping in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import our needed libraries for scraping the Wikipedia article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below will do all of the web scraping. It takes in a single parameter, `wiki_url`, the URL of the Wikipedia article, which you'll be able to change in the next cell. We first use `urllib` to open the HTML of the webpage, and then use `Beautiful Soup` to parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(wiki_url):\n",
    "    # sends a urlopen request to the wikipedia article and reads the html\n",
    "    scraped_data = urllib.request.urlopen(wiki_url)\n",
    "    article = scraped_data.read()\n",
    "\n",
    "    # gets all of the HTML paragraphs from the Wikipedia article\n",
    "    return bs.BeautifulSoup(article, \"lxml\").find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to change the URL!\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "wiki_paragraphs = get_data(wiki_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `format_paragraphs()` below. Most of the method is filled out for you, but you need to loops through each paragraph in `paragraphs`, and append its `.text` value to `text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Loop through each paragraph by writing <code>for paragraph in paragraphs</code></li>\n",
    "    <li>Append to <code>text</code> by adding the value of <code>paragraph.text</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Solution</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <code>def format_paragraphs(paragraphs):\n",
    "    text = \"\"\n",
    "    # append all the paragraphs\n",
    "    for paragraph in paragraphs:\n",
    "        text += paragraph.text\n",
    "    # return the cleaned text\n",
    "    return clean_text(text)</code>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_paragraphs(paragraphs):\n",
    "    text = \"\"\n",
    "    # append all the paragraphs\n",
    "    for paragraph in paragraphs:\n",
    "        text += paragraph.text\n",
    "    \n",
    "    # return the cleaned text\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # get rid of useless characters (references and extra spaces)\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to get the formatted article! (print it if you like)\n",
    "wiki_article = format_paragraphs(wiki_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating a Model\n",
    "\n",
    "The second step in our pipeline is create the model for this project, a table of weighted frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using `nltk`, the natural language toolkit library, to tokenize our text throughout this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin tokenizing, let's store our `stopwords`. If you're feeling **spicy** you can switch the language to something else and find a wikipedia article in a different language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `build_freq_dict()`, which does the following:\n",
    "1. Tokenizes the words of the articles using `tokenizer.tokenize()`. You can read more about the method [here](https://docs.python.org/3/library/tokenize.html)\n",
    "2. Finds the appropriate `frequency` if it's in the `freqs` dictionary, or 0 if it's not in the `freqs` dictionary\n",
    "3. Increments `frequency` by 1\n",
    "4. Updates `freqs` with the new `frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq_dict(article):\n",
    "    # initialize the dictionary\n",
    "    freqs = dict()\n",
    "    \n",
    "    # tokenizes the words\n",
    "    article = article.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_words = tokenizer.tokenize(article)\n",
    "    \n",
    "    # iterates through each token\n",
    "    for word in tokenized_words:\n",
    "        # skips stopwords\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        \n",
    "        # gets the frequency\n",
    "        frequency = freqs.get(word, 0)\n",
    "        # increments the frequency by 1\n",
    "        frequency += 1\n",
    "        # updates the frequency\n",
    "        freqs[word] = frequency\n",
    "    \n",
    "    # return the dictionary\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoo Wee Mama!\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test your method\n",
    "test_article = \"\"\"In the night, I hear them talk the coldest story ever told\n",
    "                Somewhere far along this road, he lost his soul\n",
    "                To a woman so heartless\n",
    "                How could you be so heartless\n",
    "                Oh, how could you be so heartless?\"\"\"\n",
    "\n",
    "test_dict = build_freq_dict(test_article)\n",
    "\n",
    "actual_dict = {'night': 1, 'hear': 1, 'talk': 1, 'coldest': 1, 'story': 1, 'ever': 1, 'told': 1,\n",
    "               'somewhere': 1, 'far': 1, 'along': 1, 'road': 1, 'lost': 1, 'soul': 1, 'woman': 1,\n",
    "               'heartless': 3, 'could': 2, 'oh': 1}\n",
    "\n",
    "if (test_dict != actual_dict):\n",
    "    print(\"Sorry homeboy/homegirl\")\n",
    "else:\n",
    "    print(\"Zoo Wee Mama!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you passed the test case above, you can run the cell below to store the frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = build_freq_dict(wiki_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our frequency dictionary, we need to calculate the weighted frequency of occurrence\n",
    "\n",
    "We use the following formula to obtain the correct value:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{300}&space;ratio_i&space;=&space;\\frac{frequency_i}{max_{frequency}}\" title=\"ratio_i = \\frac{frequency_i}{max_{frequency}}\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `build_ratio_dict()`, which converts our dictionary of frequencies into a dictionary of occurence weights as follows:\n",
    "\n",
    "1. Set `frequencies` equal to a `numpy array` of the `values()` of `freqs`\n",
    "2. Finds the maximum frequency in `frequencies`\n",
    "3. Calculates `ratios` according to the formula above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ratio_dict(freqs):\n",
    "    # gets the frequencies\n",
    "    frequencies = np.array(list(freqs.values()))\n",
    "    # finds the maximum frequency\n",
    "    max_frequency = max(freqs.values())\n",
    "    # calculates the ratios\n",
    "    ratios = frequencies / max_frequency\n",
    "    \n",
    "    # returns the appropriate dictionary\n",
    "    return dict(zip(freqs.keys(), ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aw yea B)\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test your code\n",
    "test_ratio_dict = build_ratio_dict(test_dict)\n",
    "\n",
    "actual_dict =  {'night': 0.3333333333333333, 'hear': 0.3333333333333333, 'talk': 0.3333333333333333, \n",
    "                'coldest': 0.3333333333333333, 'story': 0.3333333333333333, 'ever': 0.3333333333333333, \n",
    "                'told': 0.3333333333333333, 'somewhere': 0.3333333333333333, 'far': 0.3333333333333333, \n",
    "                'along': 0.3333333333333333, 'road': 0.3333333333333333, 'lost': 0.3333333333333333, \n",
    "                'soul': 0.3333333333333333, 'woman': 0.3333333333333333, 'heartless': 1.0, \n",
    "                'could': 0.6666666666666666, 'oh': 0.3333333333333333}\n",
    "\n",
    "if (actual_dict != test_ratio_dict):\n",
    "    print(\"Awful, gross, yuck!\")\n",
    "else:\n",
    "    print(\"aw yea B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your function is working, run the cell below to store your dictionary of weighted occurence ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_dict = build_ratio_dict(freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Calculating Sentence Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the weight of each word, we can find the weight of the entire sentences in the article. The sentences with a higher weight will be the best at summarizing our text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by taking our original article, tokenize it by sentences, and storing it in `wiki_sentences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = nltk.sent_tokenize(wiki_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also initialize some constants to use for factoring in the length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_limit = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `calc_sentence_weight()`, which calculates the sentence weight as follows:\n",
    "1. Set `tokenized_words` using the `tokenizer.tokenize()` library function\n",
    "2. Get the `weight` from `ratio_dict`. If the current `word` is not in the dictionary, set `weight` equal to 0\n",
    "3. Add the value of `weight` to `total_weight`\n",
    "\n",
    "*Note: At the end we factor in the total length of the sentence to prioritize shorter sentences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sentence_weight(sentence, ratio_dict):\n",
    "    # intiializes the total weight to 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    # tokenizes the words\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentence = sentence.lower()\n",
    "    tokenized_words = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # edge cases\n",
    "    if (len(tokenized_words) == 0 or len(tokenized_words) >= length_limit):\n",
    "        return 0\n",
    "    \n",
    "    # iterates through each token\n",
    "    for word in tokenized_words:\n",
    "        \n",
    "        # skips stopwords\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        \n",
    "        # finds the weight in the dictionary, 0 if it doesn't exist\n",
    "        weight = ratio_dict.get(word, 0)\n",
    "        # adds this value to the total weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    \n",
    "    return total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~:)\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test your function\n",
    "test_sentence = \"In the night I hear them talk\"\n",
    "test_weight = calc_sentence_weight(test_sentence, test_ratio_dict)\n",
    "\n",
    "if (test_weight != 1):\n",
    "    print(\"Looks like something went wrong, Buster\")\n",
    "else:\n",
    "    print(\"~:)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our helper function is working, we can create a list of weights, one for each sentence in our Wikipedia article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function `create_sentence_weights()`, which uses `calc_sentence_weight()` to make a list of sentence weights as follows:\n",
    "\n",
    "1. Iterates through each `sentence` in `sentences`\n",
    "2. Calculates the `weight` using `calc_sentence_weight()`\n",
    "3. Appends `weight` to the list of `weights` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_weights(sentences, ratio_dict):\n",
    "    weights = []\n",
    "    for sentence in sentences:\n",
    "        weight = calc_sentence_weight(sentence, ratio_dict)\n",
    "        weights.append(weight)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot diggity dog!\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test your method\n",
    "test_sentences = [\"Because I'm heartless\", \"It's the coldest outside this night!\"]\n",
    "test_weights = create_sentence_weights(test_sentences, test_ratio_dict)\n",
    "\n",
    "if (test_weights != [1.0, 0.6666666666666666]):\n",
    "    print(\"Your method is kinda smelly rn ngl\")\n",
    "else:\n",
    "    print(\"Hot diggity dog!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get a list of weights for our article!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_weights = create_sentence_weights(wiki_sentences, ratio_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Putting Together a Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put the summary together, we're going to grab the `num_sentences` highest sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitialize our values\n",
    "num_sentences = 5\n",
    "wiki_weights = np.array(wiki_weights)\n",
    "wiki_sentences = np.array(wiki_sentences)\n",
    "# gets the \"num_sentences\" highest scores\n",
    "largest_weight_indexes = wiki_weights.argsort()[-num_sentences:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll concatenate all of these sentences to make a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorts them \"chronologically\", so that it makes more sense\n",
    "largest_weight_indexes.sort()\n",
    "# retrieves the highest-scoring sentences\n",
    "summary_sentences = wiki_sentences[largest_weight_indexes]\n",
    "# joins the sentences\n",
    "summary = \" \".join(summary_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can print our summary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
