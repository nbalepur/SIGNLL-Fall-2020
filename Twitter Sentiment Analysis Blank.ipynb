{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9/17 : Twitter Sentiment Analysis\n",
    "\n",
    "Hi everyone! In this notebook, we'll take a look at **sentiment analysis**. Specifically, we'll see how we can predict the emotion of a tweet!\n",
    "\n",
    "In this notebook, we have the following methods for you to fill out:\n",
    "1. `add_tweet()`\n",
    "2. `extract_features()`\n",
    "3. `get_accuracy()`\n",
    "4. `predict_tweet()`\n",
    "\n",
    "We'll start with our needed imports\n",
    "\n",
    "You may notice that some of these imports are different from the ones we usually use. `numpy` is, of course, the library we are most familiar with. `pandas` is a library for data management (similar to SQL), while `nltk` is a library specifically for NLP. The other libraries are not as important, they just help us accomplish random tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "# features from nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll download the data we need and set our work directory (so we don't have to download these files every time we open the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's format our data properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# separate our data into negative and positive labels\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "# separate our data into training and testing sets (remember, around 80/20)\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that our data is formatted, we can follow our steps for sentiment analysis:\n",
    "\n",
    "1. `Process` our tweets\n",
    "2. `Build` our dictionary\n",
    "3. `Train` our model\n",
    "4. `Test` our model\n",
    "\n",
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: `Processing` our tweets\n",
    "\n",
    "We have this method filled out for you, since almost all of it uses `nltk` functions and it can get real confusing real fast\n",
    "\n",
    "Basically, it performs all the steps we discussed in the slides, including: removing `punctuation` and `stopwords`, `tokenizing` words, and removing random `twitter symbols` (like @ and #)\n",
    "\n",
    "Feel free to look at it more closely if you want to see specifically how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        # remove stopwords and punctuation\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            # stem the word and add it to our list\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "    \n",
    "    # return our tweets\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see what happens to the given tweet after it's processed\n",
    "\n",
    "Try it out with your own tweet!\n",
    "\n",
    "*Fun fact: the Kanye tweet below might be my favorite tweet of all time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"./kanye_tweet.jpg\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try your own tweet here!\n",
    "sample_processed_tweet = \"I'm nice at ping pong\"\n",
    "print(process_tweet(sample_processed_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: `Building` our dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `add_tweet()` below, which takes a tweet and adds it to the dictionary appropriately\n",
    "\n",
    "Here are the general steps for the function:\n",
    "1. Turn the tweet into a list of tokens, called `tokenized_tweet`\n",
    "2. Set `freqs` to the value of the dictionary if `token` is in the dictionary, or [0, 0] if not in the dictionary\n",
    "3. Increment the appropriate count given the sentiment (which is 0 or 1)\n",
    "4. Update the dictionary\n",
    "\n",
    "You will need to complete steps 1-3 of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>tokenized_tweet</code> can be calculated by using our <code>process_tweet()</code> function</li>\n",
    "    <li>Look at the arguments for <code>process_tweet()</code> and make sure your inputting the right parameters!</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>If you're feeling <b>extra</b> saucy, you can combine all the steps using the dictionary's <code>get()</code> method with the <code>default</code> paramter</li>\n",
    "    <li>The code above can be executed with <code>dictionary.get(token, [0, 0])</code></li>\n",
    "</ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can use the value of <code>sentiment</code> to appropriately increment <code>freqs</code></li>\n",
    "    <li>This can be done in one line: <code>freqs[sentiment] += 1</code></li>\n",
    "</ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tweet(dictionary, tweet, sentiment):\n",
    "    # Step 1: creates the tokens from the tweet\n",
    "    tokenized_tweet = ...\n",
    "    # iterates through each token\n",
    "    for token in tokenized_tweet:\n",
    "        # Step 2: if the token is in the dictionary, freqs is that value, [0, 0] otherwise\n",
    "        freqs = ...\n",
    "        # Step 3: increases the appropriate frequency\n",
    "        ...\n",
    "        # Step 4: updates the dictionary\n",
    "        dictionary.update({token : freqs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a method to add to the dictionary for an individual tweet, we can loop through every tweet and call `add_tweet()` to build our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tweets, sentiments):\n",
    "    # initialize the dictionary\n",
    "    tweet_dict = dict({})\n",
    "    # iterate through each tweet, and add it to the dictionary\n",
    "    for tweet_num in range(len(tweets)):\n",
    "        add_tweet(tweet_dict, tweets[tweet_num], int(sentiments[tweet_num][0]))\n",
    "    # return the dictionary\n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store our dictionary of words as `tweet_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict = build_dict(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: `Training` our model\n",
    "\n",
    "Before we run gradient descent, let's define the functions that we used last week, namely:\n",
    "1. `sigmoid()` - our function for mapping values between 0 and 1\n",
    "2. `cost()` - the cost of the logistic regression\n",
    "3. `cost_derivative()` - the derivative of the cost function\n",
    "\n",
    "If you want to learn more about how these functions work/why we need them, you can look at the materials from last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    sigmoid_val = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_pred, y_actual, m):\n",
    "    cost = (-1 / m) * np.sum(y_actual.T @ np.log(y_pred) + (1 - y_actual).T @ np.log(1 - y_pred))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_derivative(predicted, actual, inputs, m):\n",
    "    derivative = (1 / m) * (inputs.T @ (predicted - actual))\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train our model, we need to create a function to properly represent our words\n",
    "\n",
    "Recall that each tweet can be represented as:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{300}&space;\\begin{Bmatrix}&space;1&space;&&space;n_{neg}&space;&&space;n_{pos}&space;\\end{Bmatrix}\" title=\"\\begin{Bmatrix} 1 & n_{neg} & n_{pos} \\end{Bmatrix}\" />\n",
    "\n",
    "\n",
    "Where $n_{neg}$ is the number of times the words in the tweet appeared in a `negative` tweet, \n",
    "\n",
    "and $n_{pos}$ is the number of times the words in the tweet appeared in a `positive` tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function `extract_features()`, which transforms the tweet according to the instructions in the above cell\n",
    "\n",
    "Here are the steps for the function:\n",
    "\n",
    "1. Create our list of tokens, `processed_tweet`\n",
    "2. Retrieve `freqs` from the dictionary\n",
    "3. Increment each element of `tweet_val` by the correct value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>processed_tweet</code> can be calculated by using our <code>process_tweet()</code> function</li>\n",
    "    <li>Look at the arguments for <code>process_tweet()</code> and make sure your inputting the right parameters!</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>If you're feeling <b>extra</b> saucy, you can combine all the steps using the dictionary's <code>get()</code> method with the <code>default</code> paramter</li>\n",
    "    <li>The code above can be executed with <code>dictionary.get(token, [0, 0])</code></li>\n",
    "</ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>tweet_val[1]</code> represents the total negative count, while <code>tweet_val[2]</code> represents the total positive count </li>\n",
    "    <li>The negative count should be incremented by <code>freqs[0]</code>, while the positive count should be incremented by <code>freqs[1]</code></li>\n",
    "</ul>\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dictionary, tweet):\n",
    "    # Step 1: Create the list of tokens\n",
    "    processed_tweet = ...\n",
    "    tweet_val = [1, 0, 0]\n",
    "    for word in processed_tweet:\n",
    "        # Step 2: Get the freqs list from the dictionary\n",
    "        freqs = ...\n",
    "        # Step 3: Increment each element of tweet_val appropriately\n",
    "        tweet_val[1] += ...\n",
    "        tweet_val[2] += ...\n",
    "    return tweet_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use `extract_features()` to build a training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_set(dictionary, tweets):\n",
    "    tweet_set = []\n",
    "    for tweet in tweets:\n",
    "        tweet_val = extract_features(dictionary, tweet)\n",
    "        tweet_set.append(tweet_val)\n",
    "    return 1.0 * np.array(tweet_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use these functions to create a test set for our logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets our training and testing sets for logistic regression\n",
    "training_x = build_set(tweet_dict, train_x)\n",
    "training_y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is finally correctly formatted, so we can begin `logistic regression`!\n",
    "\n",
    "We'll start by defining our constants for `logistic regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for gradient descent (mess around with them if you dare)\n",
    "learning_rate = 0.000001\n",
    "num_iterations = 15000\n",
    "m = training_x.shape[0]\n",
    "# initialize our thetas\n",
    "thetas = np.zeros((3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you filled out the gradient descent function last week (hopefully!), we'll fill out the method for you this time\n",
    "\n",
    "To understand more about how this algorithm works, feel free to look at the materials from last week!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(x, actual_y, thetas, learning_rate, m, num_iterations):\n",
    "    # perform the algorithm for the specified number of iterations\n",
    "    for iteration in range(num_iterations):\n",
    "        # calculate our sigmoided predicted output\n",
    "        pred_output = sigmoid(x @ thetas)\n",
    "        # get the derivative of this value\n",
    "        gradients = cost_derivative(pred_output, actual_y, x, m)\n",
    "        # adjust our thetas\n",
    "        thetas = thetas - learning_rate * gradients\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready, so let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_thetas = grad_descent(training_x, training_y, thetas, learning_rate, m, num_iterations)\n",
    "print(\"Cost: {0}\".format(cost(sigmoid(training_x @ thetas), training_y, m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: `Testing` our model\n",
    "\n",
    "Before we put in our own custom tweets, let's test our model using the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function `get_accuracy()`, which returns the accuracy of our trained model\n",
    "\n",
    "Accurancy can be defined as:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\dpi{200}&space;accuracy&space;=&space;\\frac{n_{correct}}{n_{total}}\" title=\"accuracy = \\frac{n_{correct}}{n_{total}}\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>predicted_values</code> should be 0 when our sigmoided matrix multiplication is less than 0.5, and 1 when it's greater than 0.5</li>\n",
    "    <li>Set <code>predicted_values</code> equal to <code>sigmoid(test_set @ thetas) > 0.5</code></li>\n",
    "    <li>The number of correct values is the <code>sum</code> of element-wise equivalent values of <code>predicted_values</code> and <code>y</code></li>\n",
    "    <li>accuracy can be found using <code>num_correct</code> and <code>num_total</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dictionary, x, y, thetas, num_total):\n",
    "    # create the test set\n",
    "    test_set = build_set(dictionary, x)\n",
    "    # get the predicted values\n",
    "    predicted_values = ... \n",
    "    # check how many of these values are correct\n",
    "    num_correct = ...\n",
    "    # calculate and return the accuracy\n",
    "    accuracy = ...\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test the model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = format(get_accuracy(tweet_dict, test_x, test_y, tweet_thetas, test_y.shape[0]))\n",
    "print(\"Model accuracy: {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a model accuracy over 95%, that's pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4b: `Testing` with `custom tweets`\n",
    "\n",
    "Now, let's use our model to predict our own custom tweets!\n",
    "\n",
    "Complete the function `predict_tweet()` below so we can predict our own custom tweets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>features</code> can be found by using our <code>extract_features</code> method</li>\n",
    "    <li><code>prediction</code> is equal to <code>sigmoid(features @ thetas)</code></li>\n",
    "    <li><code>return</code> whatever you want! Just remember that a <code>positive sentiment</code> occurs when <code>prediction > 0.5</code> and a <code>negative sentiment</code> otherwise</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(dictionary, tweet, thetas):\n",
    "    features = ...\n",
    "    prediction = ...\n",
    "    return ... if ... else ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function below with your own inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"covfefe\"\n",
    "print(predict_tweet(tweet_dict, tweet, tweet_thetas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice some things about the model. It does not do well with negations (words like not) and more human-like conversation, like sarcasm. This is to be expected with a fairly simple model like this, but it can be fixed with more complex algorithms and fine-tuned parameters!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
